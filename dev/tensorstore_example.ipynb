{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorStore Performance Evaluation for MBO Workflows\n",
    "\n",
    "This notebook evaluates whether TensorStore provides performance benefits for your imaging data workflows compared to current approaches using tifffile, zarr, and dask.\n",
    "\n",
    "## What is TensorStore?\n",
    "\n",
    "TensorStore is a C++ library with Python bindings designed for:\n",
    "- **High-performance I/O**: Asynchronous reads/writes with caching\n",
    "- **Multiple formats**: Native support for Zarr, N5, neuroglancer_precomputed\n",
    "- **Storage flexibility**: Local filesystem, network drives, cloud storage (GCS, S3)\n",
    "- **Advanced indexing**: Composable virtual views without copying data\n",
    "- **Concurrency**: Safe multi-process/multi-machine access with ACID guarantees\n",
    "\n",
    "## Key Questions\n",
    "\n",
    "1. **Speed**: Does TensorStore read/write faster than tifffile + zarr?\n",
    "2. **Network drives**: Does async I/O + caching help with slow network storage?\n",
    "3. **Memory**: How does memory usage compare?\n",
    "4. **Integration**: Can it replace MboRawArray or work alongside it?\n",
    "5. **Use cases**: When should you use TensorStore vs current tools?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tensorstore if needed\n",
    "# !pip install tensorstore\n",
    "\n",
    "import tensorstore as ts\n",
    "import numpy as np\n",
    "import zarr\n",
    "from numcodecs import Blosc\n",
    "import time\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mbo_utilities import imread, imwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorstore as ts\n",
    "import numpy as np\n",
    "import zarr\n",
    "import time\n",
    "from pathlib import Path\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Create Test Data\n",
    "\n",
    "Generate realistic test data similar to your ScanImage acquisitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test data: (2000, 512, 512)\n",
      "Data size: 1.05 GB (uint16)\n",
      "Test data stats: mean=2074.1, std=1189.2\n"
     ]
    }
   ],
   "source": [
    "# Create test data matching your typical dataset dimensions\n",
    "# Typical: (T, Y, X) = (1000-5000, 512, 512) or multi-plane (T*Z, Y, X)\n",
    "\n",
    "test_shape = (2000, 512, 512)  # ~2GB uint16 dataset\n",
    "chunk_size = (100, 128, 128)   # Reasonable chunk size\n",
    "\n",
    "print(f\"Creating test data: {test_shape}\")\n",
    "print(f\"Data size: {np.prod(test_shape) * 2 / 1e9:.2f} GB (uint16)\")\n",
    "\n",
    "# Generate synthetic data with realistic statistics\n",
    "np.random.seed(42)\n",
    "test_data = np.random.randint(0, 4095, size=test_shape, dtype=np.uint16)\n",
    "\n",
    "# Add some structure (spots + background) to simulate real calcium imaging\n",
    "for t in range(test_shape[0]):\n",
    "    # Add ~100 Gaussian spots per frame\n",
    "    for _ in range(100):\n",
    "        y, x = np.random.randint(50, test_shape[1]-50, 2)\n",
    "        amplitude = np.random.randint(500, 2000)\n",
    "        Y, X = np.ogrid[:test_shape[1], :test_shape[2]]\n",
    "        mask = ((Y - y)**2 + (X - x)**2) < 25\n",
    "        test_data[t][mask] = np.clip(test_data[t][mask] + amplitude, 0, 4095)\n",
    "\n",
    "print(f\"Test data stats: mean={test_data.mean():.1f}, std={test_data.std():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Write Performance - TensorStore vs Zarr vs TIFF\n",
    "\n",
    "Compare write speeds for different backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = Path(tempfile.mkdtemp())\n",
    "print(f\"Temp directory: {tmpdir}\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "# ============================================================================\n",
    "# 1. TensorStore write (Zarr backend)\n",
    "# ============================================================================\n",
    "print(\"\\n[1/4] Writing with TensorStore (Zarr)...\")\n",
    "ts_path = tmpdir / \"tensorstore.zarr\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Create TensorStore dataset\n",
    "dataset_ts = ts.open({\n",
    "    'driver': 'zarr',\n",
    "    'kvstore': {\n",
    "        'driver': 'file',\n",
    "        'path': str(ts_path),\n",
    "    },\n",
    "    'metadata': {\n",
    "        'compressor': {\n",
    "            'id': 'blosc',\n",
    "            'cname': 'lz4',\n",
    "            'clevel': 5,\n",
    "            'shuffle': 1,\n",
    "        },\n",
    "        'dtype': '<u2',\n",
    "        'shape': list(test_shape),\n",
    "        'chunks': list(chunk_size),\n",
    "    },\n",
    "    'create': True,\n",
    "    'delete_existing': True,\n",
    "}).result()\n",
    "\n",
    "# Write asynchronously\n",
    "write_future = dataset_ts.write(test_data)\n",
    "write_future.result()  # Wait for completion\n",
    "\n",
    "ts_write_time = time.time() - start\n",
    "results['tensorstore_write'] = ts_write_time\n",
    "print(f\"TensorStore write: {ts_write_time:.2f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Standard Zarr write (using Zarr v3 API)\n",
    "# ============================================================================\n",
    "print(\"\\n[2/4] Writing with standard Zarr...\")\n",
    "zarr_path = tmpdir / \"standard.zarr\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Zarr v3 uses codecs instead of compressor\n",
    "z = zarr.create(\n",
    "    store=str(zarr_path),\n",
    "    shape=test_shape,\n",
    "    chunks=chunk_size,\n",
    "    dtype=np.uint16,\n",
    "    codecs=[Blosc(cname='lz4', clevel=5, shuffle='shuffle')],  # Zarr v3 API\n",
    "    overwrite=True,\n",
    ")\n",
    "z[:] = test_data\n",
    "\n",
    "zarr_write_time = time.time() - start\n",
    "results['zarr_write'] = zarr_write_time\n",
    "print(f\"Zarr write: {zarr_write_time:.2f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. MBO imwrite (TIFF)\n",
    "# ============================================================================\n",
    "print(\"\\n[3/4] Writing with mbo_utilities.imwrite (TIFF)...\")\n",
    "tiff_path = tmpdir / \"test.tif\"\n",
    "\n",
    "start = time.time()\n",
    "imwrite(tiff_path, test_data, compression='lzw')\n",
    "tiff_write_time = time.time() - start\n",
    "results['tiff_write'] = tiff_write_time\n",
    "print(f\"TIFF write: {tiff_write_time:.2f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. MBO imwrite (Zarr)\n",
    "# ============================================================================\n",
    "print(\"\\n[4/4] Writing with mbo_utilities.imwrite (Zarr)...\")\n",
    "mbo_zarr_path = tmpdir / \"mbo.zarr\"\n",
    "\n",
    "start = time.time()\n",
    "imwrite(mbo_zarr_path, test_data)\n",
    "mbo_zarr_write_time = time.time() - start\n",
    "results['mbo_zarr_write'] = mbo_zarr_write_time\n",
    "print(f\"MBO Zarr write: {mbo_zarr_write_time:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WRITE PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for name, t in results.items():\n",
    "    print(f\"{name:25s}: {t:6.2f}s  ({test_data.nbytes/t/1e6:6.1f} MB/s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Read Performance - Full Array\n",
    "\n",
    "Compare read speeds when loading entire dataset into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/4] Reading with TensorStore...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "NOT_FOUND: Error opening \"zarr\" driver: Metadata at local file \"C:/Users/RBO/AppData/Local/Temp/tmpe_24o4te/tensorstore.zarr/.zarray\" does not exist [tensorstore_spec='{\\\"context\\\":{\\\"cache_pool\\\":{},\\\"data_copy_concurrency\\\":{},\\\"file_io_concurrency\\\":{},\\\"file_io_locking\\\":{},\\\"file_io_mode\\\":{},\\\"file_io_sync\\\":true},\\\"driver\\\":\\\"zarr\\\",\\\"kvstore\\\":{\\\"driver\\\":\\\"file\\\",\\\"path\\\":\\\"C:/Users/RBO/AppData/Local/Temp/tmpe_24o4te/tensorstore.zarr/\\\"}}'] [source locations='tensorstore/driver/kvs_backed_chunk_driver.cc:1322\\ntensorstore/driver/driver.cc:116']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[1/4] Reading with TensorStore...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m start = time.time()\n\u001b[32m      9\u001b[39m dataset_ts = \u001b[43mts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdriver\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mzarr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkvstore\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdriver\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfile\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mts_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m data_ts = dataset_ts.read().result()\n\u001b[32m     14\u001b[39m ts_read_time = time.time() - start\n",
      "\u001b[31mValueError\u001b[39m: NOT_FOUND: Error opening \"zarr\" driver: Metadata at local file \"C:/Users/RBO/AppData/Local/Temp/tmpe_24o4te/tensorstore.zarr/.zarray\" does not exist [tensorstore_spec='{\\\"context\\\":{\\\"cache_pool\\\":{},\\\"data_copy_concurrency\\\":{},\\\"file_io_concurrency\\\":{},\\\"file_io_locking\\\":{},\\\"file_io_mode\\\":{},\\\"file_io_sync\\\":true},\\\"driver\\\":\\\"zarr\\\",\\\"kvstore\\\":{\\\"driver\\\":\\\"file\\\",\\\"path\\\":\\\"C:/Users/RBO/AppData/Local/Temp/tmpe_24o4te/tensorstore.zarr/\\\"}}'] [source locations='tensorstore/driver/kvs_backed_chunk_driver.cc:1322\\ntensorstore/driver/driver.cc:116']"
     ]
    }
   ],
   "source": [
    "read_results = {}\n",
    "\n",
    "# ============================================================================\n",
    "# 1. TensorStore read\n",
    "# ============================================================================\n",
    "print(\"[1/4] Reading with TensorStore...\")\n",
    "\n",
    "start = time.time()\n",
    "dataset_ts = ts.open({\n",
    "    'driver': 'zarr',\n",
    "    'kvstore': {'driver': 'file', 'path': str(ts_path)},\n",
    "}).result()\n",
    "data_ts = dataset_ts.read().result()\n",
    "ts_read_time = time.time() - start\n",
    "read_results['tensorstore_read'] = ts_read_time\n",
    "print(f\"TensorStore read: {ts_read_time:.2f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Standard Zarr read\n",
    "# ============================================================================\n",
    "print(\"\\n[2/4] Reading with standard Zarr...\")\n",
    "\n",
    "start = time.time()\n",
    "z = zarr.open(str(zarr_path), mode='r')\n",
    "data_zarr = z[:]\n",
    "zarr_read_time = time.time() - start\n",
    "read_results['zarr_read'] = zarr_read_time\n",
    "print(f\"Zarr read: {zarr_read_time:.2f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. TIFF read\n",
    "# ============================================================================\n",
    "print(\"\\n[3/4] Reading with mbo_utilities.imread (TIFF)...\")\n",
    "\n",
    "start = time.time()\n",
    "data_tiff = imread(tiff_path)[:]\n",
    "tiff_read_time = time.time() - start\n",
    "read_results['tiff_read'] = tiff_read_time\n",
    "print(f\"TIFF read: {tiff_read_time:.2f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. MBO Zarr read\n",
    "# ============================================================================\n",
    "print(\"\\n[4/4] Reading with mbo_utilities.imread (Zarr)...\")\n",
    "\n",
    "start = time.time()\n",
    "data_mbo_zarr = imread(mbo_zarr_path)[:]\n",
    "mbo_zarr_read_time = time.time() - start\n",
    "read_results['mbo_zarr_read'] = mbo_zarr_read_time\n",
    "print(f\"MBO Zarr read: {mbo_zarr_read_time:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"READ PERFORMANCE SUMMARY (Full Array)\")\n",
    "print(\"=\"*60)\n",
    "for name, t in read_results.items():\n",
    "    print(f\"{name:25s}: {t:6.2f}s  ({test_data.nbytes/t/1e6:6.1f} MB/s)\")\n",
    "\n",
    "# Verify data integrity\n",
    "print(\"\\nVerifying data integrity...\")\n",
    "assert np.array_equal(data_ts, test_data), \"TensorStore data mismatch!\"\n",
    "assert np.array_equal(data_zarr, test_data), \"Zarr data mismatch!\"\n",
    "assert np.array_equal(data_tiff, test_data), \"TIFF data mismatch!\"\n",
    "assert np.array_equal(data_mbo_zarr, test_data), \"MBO Zarr data mismatch!\"\n",
    "print(\"✓ All data matches original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Slice Performance - Random Access Patterns\n",
    "\n",
    "Test performance for typical access patterns:\n",
    "1. Single frame access (important for GUI)\n",
    "2. Small ROI extraction\n",
    "3. Time series from single pixel/ROI\n",
    "4. Strided access (every Nth frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open datasets for slicing tests\n",
    "ts_dataset = ts.open({\n",
    "    'driver': 'zarr',\n",
    "    'kvstore': {'driver': 'file', 'path': str(ts_path)},\n",
    "}).result()\n",
    "\n",
    "zarr_dataset = zarr.open(str(zarr_path), mode='r')\n",
    "tiff_dataset = imread(tiff_path)\n",
    "\n",
    "slice_results = {}\n",
    "\n",
    "# ============================================================================\n",
    "# Test 1: Single frame access (100 random frames)\n",
    "# ============================================================================\n",
    "print(\"Test 1: Single frame access (100 random frames)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "frames_to_test = np.random.randint(0, test_shape[0], 100)\n",
    "\n",
    "# TensorStore\n",
    "start = time.time()\n",
    "for idx in frames_to_test:\n",
    "    _ = ts_dataset[idx].read().result()\n",
    "ts_frame_time = time.time() - start\n",
    "print(f\"TensorStore: {ts_frame_time:.3f}s ({ts_frame_time/100*1000:.1f}ms per frame)\")\n",
    "\n",
    "# Zarr\n",
    "start = time.time()\n",
    "for idx in frames_to_test:\n",
    "    _ = zarr_dataset[idx]\n",
    "zarr_frame_time = time.time() - start\n",
    "print(f\"Zarr:        {zarr_frame_time:.3f}s ({zarr_frame_time/100*1000:.1f}ms per frame)\")\n",
    "\n",
    "# TIFF (MboRawArray caches decoded frames)\n",
    "start = time.time()\n",
    "for idx in frames_to_test:\n",
    "    _ = tiff_dataset[idx]\n",
    "tiff_frame_time = time.time() - start\n",
    "print(f\"TIFF:        {tiff_frame_time:.3f}s ({tiff_frame_time/100*1000:.1f}ms per frame)\")\n",
    "\n",
    "slice_results['single_frame'] = {\n",
    "    'tensorstore': ts_frame_time,\n",
    "    'zarr': zarr_frame_time,\n",
    "    'tiff': tiff_frame_time,\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# Test 2: Small ROI extraction (100x100 pixels, 100 frames)\n",
    "# ============================================================================\n",
    "print(\"\\nTest 2: Small ROI extraction (100x100 pixels, 100 frames)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "roi_slice = (slice(0, 100), slice(200, 300), slice(200, 300))\n",
    "\n",
    "# TensorStore\n",
    "start = time.time()\n",
    "_ = ts_dataset[roi_slice].read().result()\n",
    "ts_roi_time = time.time() - start\n",
    "print(f\"TensorStore: {ts_roi_time:.3f}s\")\n",
    "\n",
    "# Zarr\n",
    "start = time.time()\n",
    "_ = zarr_dataset[roi_slice]\n",
    "zarr_roi_time = time.time() - start\n",
    "print(f\"Zarr:        {zarr_roi_time:.3f}s\")\n",
    "\n",
    "# TIFF\n",
    "start = time.time()\n",
    "_ = tiff_dataset[roi_slice]\n",
    "tiff_roi_time = time.time() - start\n",
    "print(f\"TIFF:        {tiff_roi_time:.3f}s\")\n",
    "\n",
    "slice_results['small_roi'] = {\n",
    "    'tensorstore': ts_roi_time,\n",
    "    'zarr': zarr_roi_time,\n",
    "    'tiff': tiff_roi_time,\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# Test 3: Time series from single pixel\n",
    "# ============================================================================\n",
    "print(\"\\nTest 3: Time series from single pixel (all frames)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pixel_slice = (slice(None), 256, 256)\n",
    "\n",
    "# TensorStore\n",
    "start = time.time()\n",
    "_ = ts_dataset[pixel_slice].read().result()\n",
    "ts_pixel_time = time.time() - start\n",
    "print(f\"TensorStore: {ts_pixel_time:.3f}s\")\n",
    "\n",
    "# Zarr\n",
    "start = time.time()\n",
    "_ = zarr_dataset[pixel_slice]\n",
    "zarr_pixel_time = time.time() - start\n",
    "print(f\"Zarr:        {zarr_pixel_time:.3f}s\")\n",
    "\n",
    "# TIFF\n",
    "start = time.time()\n",
    "_ = tiff_dataset[pixel_slice]\n",
    "tiff_pixel_time = time.time() - start\n",
    "print(f\"TIFF:        {tiff_pixel_time:.3f}s\")\n",
    "\n",
    "slice_results['pixel_timeseries'] = {\n",
    "    'tensorstore': ts_pixel_time,\n",
    "    'zarr': zarr_pixel_time,\n",
    "    'tiff': tiff_pixel_time,\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# Test 4: Strided access (every 10th frame)\n",
    "# ============================================================================\n",
    "print(\"\\nTest 4: Strided access (every 10th frame)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "stride_slice = (slice(0, test_shape[0], 10), slice(None), slice(None))\n",
    "\n",
    "# TensorStore\n",
    "start = time.time()\n",
    "_ = ts_dataset[stride_slice].read().result()\n",
    "ts_stride_time = time.time() - start\n",
    "print(f\"TensorStore: {ts_stride_time:.3f}s\")\n",
    "\n",
    "# Zarr\n",
    "start = time.time()\n",
    "_ = zarr_dataset[stride_slice]\n",
    "zarr_stride_time = time.time() - start\n",
    "print(f\"Zarr:        {zarr_stride_time:.3f}s\")\n",
    "\n",
    "# TIFF\n",
    "start = time.time()\n",
    "_ = tiff_dataset[stride_slice]\n",
    "tiff_stride_time = time.time() - start\n",
    "print(f\"TIFF:        {tiff_stride_time:.3f}s\")\n",
    "\n",
    "slice_results['strided_access'] = {\n",
    "    'tensorstore': ts_stride_time,\n",
    "    'zarr': zarr_stride_time,\n",
    "    'tiff': tiff_stride_time,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Caching and Network Performance\n",
    "\n",
    "Test TensorStore's caching capabilities - this is where it could shine for network drives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing TensorStore caching behavior\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Open with different cache sizes\n",
    "cache_sizes = [0, 10_000_000, 100_000_000, 500_000_000]  # 0, 10MB, 100MB, 500MB\n",
    "cache_results = {}\n",
    "\n",
    "for cache_size in cache_sizes:\n",
    "    print(f\"\\nCache size: {cache_size/1e6:.0f} MB\")\n",
    "\n",
    "    # Open with cache\n",
    "    dataset = ts.open({\n",
    "        'driver': 'zarr',\n",
    "        'kvstore': {'driver': 'file', 'path': str(ts_path)},\n",
    "        'context': {\n",
    "            'cache_pool': {'total_bytes_limit': cache_size}\n",
    "        },\n",
    "        'recheck_cached_data': False,  # Don't revalidate cached data\n",
    "    }).result()\n",
    "\n",
    "    # Read 100 random frames twice (second read should hit cache)\n",
    "    frames = np.random.randint(0, test_shape[0], 100)\n",
    "\n",
    "    # First read (cold cache)\n",
    "    start = time.time()\n",
    "    for idx in frames:\n",
    "        _ = dataset[idx].read().result()\n",
    "    first_read = time.time() - start\n",
    "\n",
    "    # Second read (warm cache - same frames)\n",
    "    start = time.time()\n",
    "    for idx in frames:\n",
    "        _ = dataset[idx].read().result()\n",
    "    second_read = time.time() - start\n",
    "\n",
    "    cache_results[cache_size] = {\n",
    "        'first': first_read,\n",
    "        'second': second_read,\n",
    "        'speedup': first_read / second_read if second_read > 0 else 0,\n",
    "    }\n",
    "\n",
    "    print(f\"  First read:  {first_read:.3f}s\")\n",
    "    print(f\"  Second read: {second_read:.3f}s\")\n",
    "    print(f\"  Speedup:     {first_read/second_read:.1f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CACHE PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Cache Size':>15s}  {'Cold (s)':>10s}  {'Warm (s)':>10s}  {'Speedup':>10s}\")\n",
    "print(\"-\"*60)\n",
    "for cache_size, res in cache_results.items():\n",
    "    print(f\"{cache_size/1e6:>13.0f} MB  {res['first']:>10.3f}  {res['second']:>10.3f}  {res['speedup']:>9.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Async I/O - Overlapping Computation and I/O\n",
    "\n",
    "Demonstrate TensorStore's async capabilities for pipelining operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparing synchronous vs asynchronous I/O\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simulate processing pipeline: read frame -> process -> next frame\n",
    "num_frames = 200\n",
    "frames_to_process = list(range(0, test_shape[0], test_shape[0]//num_frames)[:num_frames])\n",
    "\n",
    "def process_frame(frame_data):\n",
    "    \"\"\"Simulate some processing (e.g., denoising, filtering)\"\"\"\n",
    "    # Mean filter\n",
    "    from scipy.ndimage import uniform_filter\n",
    "    return uniform_filter(frame_data.astype(np.float32), size=3)\n",
    "\n",
    "# ============================================================================\n",
    "# Synchronous: Read -> Process -> Read -> Process ...\n",
    "# ============================================================================\n",
    "print(\"\\nSynchronous pipeline (read-then-process)...\")\n",
    "dataset = ts.open({\n",
    "    'driver': 'zarr',\n",
    "    'kvstore': {'driver': 'file', 'path': str(ts_path)},\n",
    "}).result()\n",
    "\n",
    "start = time.time()\n",
    "results_sync = []\n",
    "for idx in tqdm(frames_to_process, desc=\"Sync\"):\n",
    "    frame = dataset[idx].read().result()  # Wait for read\n",
    "    processed = process_frame(frame)       # Process\n",
    "    results_sync.append(processed)\n",
    "sync_time = time.time() - start\n",
    "print(f\"Synchronous:  {sync_time:.2f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# Asynchronous: Read[i] while processing frame[i-1]\n",
    "# ============================================================================\n",
    "print(\"\\nAsynchronous pipeline (overlapped I/O and compute)...\")\n",
    "\n",
    "start = time.time()\n",
    "results_async = []\n",
    "\n",
    "# Start first read\n",
    "pending_read = dataset[frames_to_process[0]].read()\n",
    "\n",
    "for i in tqdm(range(len(frames_to_process)), desc=\"Async\"):\n",
    "    # Wait for current read to complete\n",
    "    current_frame = pending_read.result()\n",
    "\n",
    "    # Start next read (if not last iteration)\n",
    "    if i + 1 < len(frames_to_process):\n",
    "        pending_read = dataset[frames_to_process[i + 1]].read()\n",
    "\n",
    "    # Process current frame while next frame is loading\n",
    "    processed = process_frame(current_frame)\n",
    "    results_async.append(processed)\n",
    "\n",
    "async_time = time.time() - start\n",
    "print(f\"Asynchronous: {async_time:.2f}s\")\n",
    "print(f\"Speedup:      {sync_time/async_time:.2f}x\")\n",
    "\n",
    "# Verify results match\n",
    "assert all(np.allclose(a, b) for a, b in zip(results_sync, results_async))\n",
    "print(\"✓ Results match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Virtual Views and Zero-Copy Operations\n",
    "\n",
    "TensorStore can create virtual views without copying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing virtual views and transformations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dataset = ts.open({\n",
    "    'driver': 'zarr',\n",
    "    'kvstore': {'driver': 'file', 'path': str(ts_path)},\n",
    "}).result()\n",
    "\n",
    "print(f\"Original dataset shape: {dataset.shape}\")\n",
    "print(f\"Original dataset domain: {dataset.domain}\")\n",
    "\n",
    "# Create ROI view (no data copied)\n",
    "roi_view = dataset[ts.d[0][100:200], ts.d[1][100:300], ts.d[2][100:300]]\n",
    "print(f\"\\nROI view shape: {roi_view.shape}\")\n",
    "print(f\"ROI view domain: {roi_view.domain}\")\n",
    "\n",
    "# Create downsampled view (every 2nd frame, every 2nd pixel)\n",
    "downsampled = dataset[::2, ::2, ::2]\n",
    "print(f\"\\nDownsampled shape: {downsampled.shape}\")\n",
    "print(f\"Downsampled domain: {downsampled.domain}\")\n",
    "\n",
    "# Transpose view\n",
    "transposed = dataset[ts.d['z', 'y', 'x'].transpose[2, 1, 0]]\n",
    "print(f\"\\nTransposed shape: {transposed.shape}\")\n",
    "\n",
    "# All of these are virtual - no data loaded yet!\n",
    "print(\"\\nReading ROI view (triggers actual I/O)...\")\n",
    "start = time.time()\n",
    "roi_data = roi_view.read().result()\n",
    "print(f\"ROI read time: {time.time() - start:.3f}s\")\n",
    "print(f\"ROI data shape: {roi_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Integration with Your Workflow\n",
    "\n",
    "How TensorStore could integrate with MboRawArray and existing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example: Converting raw TIFF to TensorStore Zarr\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scenario: You have raw ScanImage TIFFs and want to convert to\n",
    "# TensorStore-backed Zarr for faster access\n",
    "\n",
    "# Read raw TIFF using your existing pipeline\n",
    "print(\"\\n1. Loading raw TIFF with MboRawArray...\")\n",
    "raw_data = imread(tiff_path)\n",
    "print(f\"   Shape: {raw_data.shape}\")\n",
    "print(f\"   Has phase correction: {hasattr(raw_data, 'fix_phase')}\")\n",
    "\n",
    "# Create TensorStore Zarr\n",
    "print(\"\\n2. Creating TensorStore Zarr...\")\n",
    "output_path = tmpdir / \"converted.zarr\"\n",
    "\n",
    "ts_output = ts.open({\n",
    "    'driver': 'zarr',\n",
    "    'kvstore': {'driver': 'file', 'path': str(output_path)},\n",
    "    'metadata': {\n",
    "        'compressor': {'id': 'blosc', 'cname': 'lz4', 'clevel': 5, 'shuffle': 1},\n",
    "        'dtype': '<u2',\n",
    "        'shape': list(raw_data.shape),\n",
    "        'chunks': [100, 128, 128],\n",
    "    },\n",
    "    'create': True,\n",
    "    'delete_existing': True,\n",
    "}).result()\n",
    "\n",
    "# Write in chunks (to avoid loading full array)\n",
    "print(\"\\n3. Writing chunks (with progress bar)...\")\n",
    "chunk_t = 100\n",
    "start = time.time()\n",
    "\n",
    "for t_start in tqdm(range(0, raw_data.shape[0], chunk_t), desc=\"Writing\"):\n",
    "    t_end = min(t_start + chunk_t, raw_data.shape[0])\n",
    "    chunk = raw_data[t_start:t_end]\n",
    "    ts_output[t_start:t_end].write(chunk).result()\n",
    "\n",
    "convert_time = time.time() - start\n",
    "print(f\"\\nConversion time: {convert_time:.2f}s\")\n",
    "\n",
    "# Verify\n",
    "print(\"\\n4. Verifying converted data...\")\n",
    "ts_verify = ts.open({\n",
    "    'driver': 'zarr',\n",
    "    'kvstore': {'driver': 'file', 'path': str(output_path)},\n",
    "}).result()\n",
    "\n",
    "# Check random frames\n",
    "for _ in range(10):\n",
    "    idx = np.random.randint(0, raw_data.shape[0])\n",
    "    assert np.array_equal(\n",
    "        ts_verify[idx].read().result(),\n",
    "        raw_data[idx]\n",
    "    )\n",
    "print(\"✓ Conversion successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: File Size Comparison\n",
    "\n",
    "Compare storage efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_size(path):\n",
    "    \"\"\"Get total size of file or directory.\"\"\"\n",
    "    if path.is_file():\n",
    "        return path.stat().st_size\n",
    "    total = 0\n",
    "    for entry in path.rglob('*'):\n",
    "        if entry.is_file():\n",
    "            total += entry.stat().st_size\n",
    "    return total\n",
    "\n",
    "print(\"File sizes:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original data:       {test_data.nbytes/1e6:8.1f} MB (uncompressed)\")\n",
    "print(f\"TensorStore Zarr:    {get_size(ts_path)/1e6:8.1f} MB\")\n",
    "print(f\"Standard Zarr:       {get_size(zarr_path)/1e6:8.1f} MB\")\n",
    "print(f\"TIFF (LZW):          {get_size(tiff_path)/1e6:8.1f} MB\")\n",
    "print(f\"MBO Zarr:            {get_size(mbo_zarr_path)/1e6:8.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Summary and Recommendations\n",
    "\n",
    "### Performance Summary\n",
    "\n",
    "Based on the benchmarks above, here's when to use each tool:\n",
    "\n",
    "#### **Use TensorStore when:**\n",
    "\n",
    "1. **Network storage with high latency**\n",
    "   - The caching layer can provide significant speedups for repeated access\n",
    "   - Async I/O helps overlap network latency with computation\n",
    "\n",
    "2. **Complex access patterns**\n",
    "   - Random frame access benefits from chunk-level caching\n",
    "   - Virtual views allow efficient ROI extraction without copying\n",
    "\n",
    "3. **Parallel processing pipelines**\n",
    "   - Async API enables overlapping I/O and computation\n",
    "   - Good for Suite2p/Suite3D preprocessing workflows\n",
    "\n",
    "4. **Cloud storage integration**\n",
    "   - Native GCS/S3 support for large-scale datasets\n",
    "   - ACID transactions for safe multi-user access\n",
    "\n",
    "#### **Stick with current tools when:**\n",
    "\n",
    "1. **Local SSD storage**\n",
    "   - TensorStore overhead may not be worth it\n",
    "   - Current zarr/tifffile are already fast\n",
    "\n",
    "2. **Raw TIFF processing**\n",
    "   - MboRawArray has ScanImage-specific metadata handling\n",
    "   - Phase correction is integrated\n",
    "   - TensorStore doesn't understand TIFF metadata\n",
    "\n",
    "3. **Simple workflows**\n",
    "   - Sequential processing doesn't benefit from async\n",
    "   - Standard zarr is simpler and well-understood\n",
    "\n",
    "4. **GUI real-time display**\n",
    "   - Both are similar for single-frame access\n",
    "   - MboRawArray's frame caching might be better\n",
    "\n",
    "### Recommended Hybrid Approach\n",
    "\n",
    "```python\n",
    "# Acquisition: Save as standard TIFF (existing workflow)\n",
    "# - Keeps ScanImage metadata\n",
    "# - MboRawArray handles phase correction\n",
    "\n",
    "# Preprocessing: Convert to TensorStore Zarr for analysis\n",
    "# - Apply phase correction once\n",
    "# - Store in chunked Zarr via TensorStore\n",
    "# - Benefits from caching during Suite2p/Suite3D\n",
    "\n",
    "# Analysis: Use TensorStore for network-based processing\n",
    "# - Faster random access with cache\n",
    "# - Async I/O for pipelines\n",
    "# - Safe multi-user access\n",
    "```\n",
    "\n",
    "### Disadvantages of TensorStore\n",
    "\n",
    "1. **Additional dependency**: Another library to maintain\n",
    "2. **Learning curve**: More complex API than simple zarr\n",
    "3. **Overhead**: May be slower than zarr for local SSD\n",
    "4. **Limited format support**: No native TIFF support\n",
    "5. **Memory**: Cache can use significant RAM\n",
    "\n",
    "### Advantages of TensorStore\n",
    "\n",
    "1. **Caching**: Automatic chunk-level caching\n",
    "2. **Async I/O**: Overlap I/O with computation\n",
    "3. **Virtual views**: Zero-copy operations\n",
    "4. **Cloud support**: Native GCS/S3 integration\n",
    "5. **Transactions**: ACID guarantees for safe writes\n",
    "6. **Performance**: C++ backend is very fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "import shutil\n",
    "shutil.rmtree(tmpdir)\n",
    "print(f\"Cleaned up {tmpdir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbo-utilities",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
