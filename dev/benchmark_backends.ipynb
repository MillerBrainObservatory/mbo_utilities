{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backend Benchmark: PySide6 vs GLFW\n",
    "\n",
    "This notebook benchmarks import and load times for different GUI backends with `run_gui`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T18:23:24.518837Z",
     "iopub.status.busy": "2025-11-22T18:23:24.518837Z",
     "iopub.status.idle": "2025-11-22T18:23:24.883559Z",
     "shell.execute_reply": "2025-11-22T18:23:24.882557Z",
     "shell.execute_reply.started": "2025-11-22T18:23:24.518837Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.9 environment at: C:\\Users\\RBO\\repos\\mbo_utilities\\.venv\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m93 packages\u001b[0m \u001b[2min 216ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 32ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjupyterlab-vim\u001b[0m\u001b[2m==4.1.4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install jupyterlab-vim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T18:24:20.220005Z",
     "iopub.status.busy": "2025-11-22T18:24:20.220005Z",
     "iopub.status.idle": "2025-11-22T18:24:20.233812Z",
     "shell.execute_reply": "2025-11-22T18:24:20.233812Z",
     "shell.execute_reply.started": "2025-11-22T18:24:20.220005Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T18:24:23.319254Z",
     "iopub.status.busy": "2025-11-22T18:24:23.319254Z",
     "iopub.status.idle": "2025-11-22T18:24:23.322343Z",
     "shell.execute_reply": "2025-11-22T18:24:23.322343Z",
     "shell.execute_reply.started": "2025-11-22T18:24:23.319254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test data path - update this to your test data\n",
    "TEST_DATA_PATH = r\"E:\\tests\\lbm\\mbo_utilities\\big_raw\"\n",
    "\n",
    "# Number of runs for averaging\n",
    "N_RUNS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Time Benchmarks\n",
    "\n",
    "Measure how long it takes to import the necessary modules for each backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T18:24:25.329408Z",
     "iopub.status.busy": "2025-11-22T18:24:25.328409Z",
     "iopub.status.idle": "2025-11-22T18:24:25.334911Z",
     "shell.execute_reply": "2025-11-22T18:24:25.334409Z",
     "shell.execute_reply.started": "2025-11-22T18:24:25.329408Z"
    }
   },
   "outputs": [],
   "source": [
    "def benchmark_import(backend: str, n_runs: int = 3) -> dict:\n",
    "    \"\"\"Benchmark import times for a specific backend.\"\"\"\n",
    "    \n",
    "    script = f'''\n",
    "import time\n",
    "import os\n",
    "os.environ[\"WGPU_GUI_BACKEND\"] = \"{backend}\"\n",
    "\n",
    "start = time.perf_counter()\n",
    "import wgpu\n",
    "wgpu_time = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "import fastplotlib as fpl\n",
    "fpl_time = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "import mbo_utilities as mbo\n",
    "mbo_time = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "from mbo_utilities.graphics import run_gui\n",
    "run_gui_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"wgpu:{{wgpu_time:.4f}}\")\n",
    "print(f\"fpl:{{fpl_time:.4f}}\")\n",
    "print(f\"mbo:{{mbo_time:.4f}}\")\n",
    "print(f\"run_gui:{{run_gui_time:.4f}}\")\n",
    "'''\n",
    "    \n",
    "    times = {\"wgpu\": [], \"fpl\": [], \"mbo\": [], \"run_gui\": []}\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-c\", script],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            env={**dict(__import__(\"os\").environ), \"WGPU_GUI_BACKEND\": backend}\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"Error on run {i+1}: {result.stderr}\")\n",
    "            continue\n",
    "            \n",
    "        for line in result.stdout.strip().split(\"\\n\"):\n",
    "            key, val = line.split(\":\")\n",
    "            times[key].append(float(val))\n",
    "    \n",
    "    # Calculate averages\n",
    "    return {\n",
    "        \"backend\": backend,\n",
    "        \"wgpu_avg\": sum(times[\"wgpu\"]) / len(times[\"wgpu\"]) if times[\"wgpu\"] else 0,\n",
    "        \"fpl_avg\": sum(times[\"fpl\"]) / len(times[\"fpl\"]) if times[\"fpl\"] else 0,\n",
    "        \"mbo_avg\": sum(times[\"mbo\"]) / len(times[\"mbo\"]) if times[\"mbo\"] else 0,\n",
    "        \"run_gui_avg\": sum(times[\"run_gui\"]) / len(times[\"run_gui\"]) if times[\"run_gui\"] else 0,\n",
    "        \"total_avg\": sum(sum(v) for v in times.values()) / n_runs if any(times.values()) else 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T18:24:28.944581Z",
     "iopub.status.busy": "2025-11-22T18:24:28.944581Z",
     "iopub.status.idle": "2025-11-22T18:24:41.109891Z",
     "shell.execute_reply": "2025-11-22T18:24:41.109891Z",
     "shell.execute_reply.started": "2025-11-22T18:24:28.944581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking GLFW imports...\n",
      "GLFW Import Times (avg of 3 runs):\n",
      "  wgpu:    0.0975s\n",
      "  fpl:     1.9428s\n",
      "  mbo:     1.6063s\n",
      "  run_gui: 0.0185s\n",
      "  TOTAL:   3.6651s\n"
     ]
    }
   ],
   "source": [
    "print(\"Benchmarking GLFW imports...\")\n",
    "glfw_import = benchmark_import(\"glfw\", N_RUNS)\n",
    "print(f\"GLFW Import Times (avg of {N_RUNS} runs):\")\n",
    "print(f\"  wgpu:    {glfw_import['wgpu_avg']:.4f}s\")\n",
    "print(f\"  fpl:     {glfw_import['fpl_avg']:.4f}s\")\n",
    "print(f\"  mbo:     {glfw_import['mbo_avg']:.4f}s\")\n",
    "print(f\"  run_gui: {glfw_import['run_gui_avg']:.4f}s\")\n",
    "print(f\"  TOTAL:   {glfw_import['total_avg']:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T18:25:13.021138Z",
     "iopub.status.busy": "2025-11-22T18:25:13.021138Z",
     "iopub.status.idle": "2025-11-22T18:25:25.064665Z",
     "shell.execute_reply": "2025-11-22T18:25:25.064665Z",
     "shell.execute_reply.started": "2025-11-22T18:25:13.021138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking PySide6 imports...\n",
      "PySide6 Import Times (avg of 3 runs):\n",
      "  wgpu:    0.0808s\n",
      "  fpl:     1.9347s\n",
      "  mbo:     1.5933s\n",
      "  run_gui: 0.0188s\n",
      "  TOTAL:   3.6277s\n"
     ]
    }
   ],
   "source": [
    "print(\"Benchmarking PySide6 imports...\")\n",
    "pyside6_import = benchmark_import(\"pyside6\", N_RUNS)\n",
    "print(f\"PySide6 Import Times (avg of {N_RUNS} runs):\")\n",
    "print(f\"  wgpu:    {pyside6_import['wgpu_avg']:.4f}s\")\n",
    "print(f\"  fpl:     {pyside6_import['fpl_avg']:.4f}s\")\n",
    "print(f\"  mbo:     {pyside6_import['mbo_avg']:.4f}s\")\n",
    "print(f\"  run_gui: {pyside6_import['run_gui_avg']:.4f}s\")\n",
    "print(f\"  TOTAL:   {pyside6_import['total_avg']:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Load + Widget Creation Benchmarks\n",
    "\n",
    "Measure how long it takes to load data and create the ImageWidget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T18:25:38.261050Z",
     "iopub.status.busy": "2025-11-22T18:25:38.259045Z",
     "iopub.status.idle": "2025-11-22T18:25:38.266417Z",
     "shell.execute_reply": "2025-11-22T18:25:38.266417Z",
     "shell.execute_reply.started": "2025-11-22T18:25:38.261050Z"
    }
   },
   "outputs": [],
   "source": [
    "def benchmark_widget_creation(backend: str, data_path: str, n_runs: int = 3) -> dict:\n",
    "    \"\"\"Benchmark widget creation times for a specific backend.\"\"\"\n",
    "    \n",
    "    script = f'''\n",
    "import time\n",
    "import os\n",
    "os.environ[\"WGPU_GUI_BACKEND\"] = \"{backend}\"\n",
    "\n",
    "# Import phase\n",
    "import_start = time.perf_counter()\n",
    "import mbo_utilities as mbo\n",
    "from mbo_utilities.graphics.run_gui import _create_image_widget\n",
    "import_time = time.perf_counter() - import_start\n",
    "\n",
    "# Data load phase\n",
    "load_start = time.perf_counter()\n",
    "data = mbo.imread(r\"{data_path}\")\n",
    "load_time = time.perf_counter() - load_start\n",
    "\n",
    "# Widget creation phase (without showing)\n",
    "widget_start = time.perf_counter()\n",
    "import fastplotlib as fpl\n",
    "import numpy as np\n",
    "from mbo_utilities.graphics._processors import MboImageProcessor\n",
    "\n",
    "ndim = data.ndim\n",
    "if ndim == 4:\n",
    "    slider_dim_names = (\"t\", \"z\")\n",
    "    window_funcs = (np.mean, None)\n",
    "    window_sizes = (1, None)\n",
    "elif ndim == 3:\n",
    "    slider_dim_names = (\"t\",)\n",
    "    window_funcs = (np.mean,)\n",
    "    window_sizes = (1,)\n",
    "else:\n",
    "    slider_dim_names = None\n",
    "    window_funcs = None\n",
    "    window_sizes = None\n",
    "\n",
    "iw = fpl.ImageWidget(\n",
    "    data=data,\n",
    "    processors=MboImageProcessor,\n",
    "    slider_dim_names=slider_dim_names,\n",
    "    window_funcs=window_funcs,\n",
    "    window_sizes=window_sizes,\n",
    "    histogram_widget=True,\n",
    "    figure_kwargs={{\"size\": (800, 800)}},\n",
    "    graphic_kwargs={{\"vmin\": -100, \"vmax\": 4000}},\n",
    ")\n",
    "widget_time = time.perf_counter() - widget_start\n",
    "\n",
    "# Show phase\n",
    "show_start = time.perf_counter()\n",
    "iw.show()\n",
    "show_time = time.perf_counter() - show_start\n",
    "\n",
    "# First frame render\n",
    "render_start = time.perf_counter()\n",
    "iw.figure.canvas.draw()\n",
    "render_time = time.perf_counter() - render_start\n",
    "\n",
    "# Cleanup\n",
    "try:\n",
    "    iw.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"import:{{import_time:.4f}}\")\n",
    "print(f\"load:{{load_time:.4f}}\")\n",
    "print(f\"widget:{{widget_time:.4f}}\")\n",
    "print(f\"show:{{show_time:.4f}}\")\n",
    "print(f\"render:{{render_time:.4f}}\")\n",
    "print(f\"shape:{{data.shape}}\")\n",
    "'''\n",
    "    \n",
    "    times = {\"import\": [], \"load\": [], \"widget\": [], \"show\": [], \"render\": []}\n",
    "    shape = None\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        print(f\"  Run {i+1}/{n_runs}...\", end=\" \")\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-c\", script],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            env={**dict(__import__(\"os\").environ), \"WGPU_GUI_BACKEND\": backend}\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"Error: {result.stderr[:200]}\")\n",
    "            continue\n",
    "        \n",
    "        print(\"done\")\n",
    "        for line in result.stdout.strip().split(\"\\n\"):\n",
    "            key, val = line.split(\":\")\n",
    "            if key == \"shape\":\n",
    "                shape = val\n",
    "            else:\n",
    "                times[key].append(float(val))\n",
    "    \n",
    "    # Calculate averages\n",
    "    return {\n",
    "        \"backend\": backend,\n",
    "        \"shape\": shape,\n",
    "        \"import_avg\": sum(times[\"import\"]) / len(times[\"import\"]) if times[\"import\"] else 0,\n",
    "        \"load_avg\": sum(times[\"load\"]) / len(times[\"load\"]) if times[\"load\"] else 0,\n",
    "        \"widget_avg\": sum(times[\"widget\"]) / len(times[\"widget\"]) if times[\"widget\"] else 0,\n",
    "        \"show_avg\": sum(times[\"show\"]) / len(times[\"show\"]) if times[\"show\"] else 0,\n",
    "        \"render_avg\": sum(times[\"render\"]) / len(times[\"render\"]) if times[\"render\"] else 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T18:38:57.802899Z",
     "iopub.status.busy": "2025-11-22T18:38:57.802899Z",
     "iopub.status.idle": "2025-11-22T18:39:21.940327Z",
     "shell.execute_reply": "2025-11-22T18:39:21.940327Z",
     "shell.execute_reply.started": "2025-11-22T18:38:57.802899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking GLFW widget creation with data: E:\\tests\\lbm\\mbo_utilities\\big_raw\n",
      "  Run 1/3... Error: \n",
      "Counting frames:   0%|          | 0/49 [00:00<?, ?it/s]\n",
      "Counting frames: 100%|##########| 49/49 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 56, in <module>\n",
      "AttributeEr\n",
      "  Run 2/3... Error: \n",
      "Counting frames:   0%|          | 0/49 [00:00<?, ?it/s]\n",
      "Counting frames: 100%|##########| 49/49 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 56, in <module>\n",
      "AttributeEr\n",
      "  Run 3/3... Error: \n",
      "Counting frames:   0%|          | 0/49 [00:00<?, ?it/s]\n",
      "Counting frames: 100%|##########| 49/49 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 56, in <module>\n",
      "AttributeEr\n",
      "\n",
      "GLFW Widget Creation Times (avg of 3 runs):\n",
      "  Data shape: None\n",
      "  Import:     0.0000s\n",
      "  Load data:  0.0000s\n",
      "  Widget:     0.0000s\n",
      "  Show:       0.0000s\n",
      "  Render:     0.0000s\n",
      "  TOTAL:      0.0000s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Benchmarking GLFW widget creation with data: {TEST_DATA_PATH}\")\n",
    "glfw_widget = benchmark_widget_creation(\"glfw\", TEST_DATA_PATH, N_RUNS)\n",
    "print(f\"\\nGLFW Widget Creation Times (avg of {N_RUNS} runs):\")\n",
    "print(f\"  Data shape: {glfw_widget['shape']}\")\n",
    "print(f\"  Import:     {glfw_widget['import_avg']:.4f}s\")\n",
    "print(f\"  Load data:  {glfw_widget['load_avg']:.4f}s\")\n",
    "print(f\"  Widget:     {glfw_widget['widget_avg']:.4f}s\")\n",
    "print(f\"  Show:       {glfw_widget['show_avg']:.4f}s\")\n",
    "print(f\"  Render:     {glfw_widget['render_avg']:.4f}s\")\n",
    "glfw_total = sum([glfw_widget['import_avg'], glfw_widget['load_avg'], \n",
    "                  glfw_widget['widget_avg'], glfw_widget['show_avg'], glfw_widget['render_avg']])\n",
    "print(f\"  TOTAL:      {glfw_total:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T18:39:21.940327Z",
     "iopub.status.busy": "2025-11-22T18:39:21.940327Z",
     "iopub.status.idle": "2025-11-22T18:39:46.365495Z",
     "shell.execute_reply": "2025-11-22T18:39:46.365495Z",
     "shell.execute_reply.started": "2025-11-22T18:39:21.940327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking PySide6 widget creation with data: E:\\tests\\lbm\\mbo_utilities\\big_raw\n",
      "  Run 1/3... Error: \n",
      "Counting frames:   0%|          | 0/49 [00:00<?, ?it/s]\n",
      "Counting frames: 100%|##########| 49/49 [00:00<?, ?it/s]\n",
      "Ignoring invalid WGPU_GUI_BACKEND 'pyside6', must be one of ['glfw', 'qt', 'jupyter', \n",
      "  Run 2/3... Error: \n",
      "Counting frames:   0%|          | 0/49 [00:00<?, ?it/s]\n",
      "Counting frames: 100%|##########| 49/49 [00:00<?, ?it/s]\n",
      "Ignoring invalid WGPU_GUI_BACKEND 'pyside6', must be one of ['glfw', 'qt', 'jupyter', \n",
      "  Run 3/3... Error: \n",
      "Counting frames:   0%|          | 0/49 [00:00<?, ?it/s]\n",
      "Counting frames: 100%|##########| 49/49 [00:00<?, ?it/s]\n",
      "Ignoring invalid WGPU_GUI_BACKEND 'pyside6', must be one of ['glfw', 'qt', 'jupyter', \n",
      "\n",
      "PySide6 Widget Creation Times (avg of 3 runs):\n",
      "  Data shape: None\n",
      "  Import:     0.0000s\n",
      "  Load data:  0.0000s\n",
      "  Widget:     0.0000s\n",
      "  Show:       0.0000s\n",
      "  Render:     0.0000s\n",
      "  TOTAL:      0.0000s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Benchmarking PySide6 widget creation with data: {TEST_DATA_PATH}\")\n",
    "pyside6_widget = benchmark_widget_creation(\"pyside6\", TEST_DATA_PATH, N_RUNS)\n",
    "print(f\"\\nPySide6 Widget Creation Times (avg of {N_RUNS} runs):\")\n",
    "print(f\"  Data shape: {pyside6_widget['shape']}\")\n",
    "print(f\"  Import:     {pyside6_widget['import_avg']:.4f}s\")\n",
    "print(f\"  Load data:  {pyside6_widget['load_avg']:.4f}s\")\n",
    "print(f\"  Widget:     {pyside6_widget['widget_avg']:.4f}s\")\n",
    "print(f\"  Show:       {pyside6_widget['show_avg']:.4f}s\")\n",
    "print(f\"  Render:     {pyside6_widget['render_avg']:.4f}s\")\n",
    "pyside6_total = sum([pyside6_widget['import_avg'], pyside6_widget['load_avg'], \n",
    "                     pyside6_widget['widget_avg'], pyside6_widget['show_avg'], pyside6_widget['render_avg']])\n",
    "print(f\"  TOTAL:      {pyside6_total:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BACKEND COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nData: {TEST_DATA_PATH}\")\n",
    "print(f\"Shape: {glfw_widget.get('shape', 'N/A')}\")\n",
    "print(f\"Runs per benchmark: {N_RUNS}\")\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Metric':<20} {'GLFW':>15} {'PySide6':>15} {'Diff':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Import times\n",
    "diff = pyside6_import['total_avg'] - glfw_import['total_avg']\n",
    "sign = \"+\" if diff > 0 else \"\"\n",
    "print(f\"{'Import (total)':<20} {glfw_import['total_avg']:>14.4f}s {pyside6_import['total_avg']:>14.4f}s {sign}{diff:>9.4f}s\")\n",
    "\n",
    "# Widget creation breakdown\n",
    "metrics = [\n",
    "    (\"Data Load\", \"load_avg\"),\n",
    "    (\"Widget Create\", \"widget_avg\"),\n",
    "    (\"Show\", \"show_avg\"),\n",
    "    (\"First Render\", \"render_avg\"),\n",
    "]\n",
    "\n",
    "for label, key in metrics:\n",
    "    g = glfw_widget.get(key, 0)\n",
    "    p = pyside6_widget.get(key, 0)\n",
    "    diff = p - g\n",
    "    sign = \"+\" if diff > 0 else \"\"\n",
    "    print(f\"{label:<20} {g:>14.4f}s {p:>14.4f}s {sign}{diff:>9.4f}s\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Total comparison\n",
    "glfw_total = glfw_import['total_avg'] + sum(glfw_widget.get(k, 0) for _, k in metrics)\n",
    "pyside6_total = pyside6_import['total_avg'] + sum(pyside6_widget.get(k, 0) for _, k in metrics)\n",
    "diff = pyside6_total - glfw_total\n",
    "sign = \"+\" if diff > 0 else \"\"\n",
    "pct = (diff / glfw_total) * 100 if glfw_total > 0 else 0\n",
    "\n",
    "print(f\"{'TOTAL':<20} {glfw_total:>14.4f}s {pyside6_total:>14.4f}s {sign}{diff:>9.4f}s\")\n",
    "print()\n",
    "\n",
    "if diff > 0:\n",
    "    print(f\"GLFW is {abs(pct):.1f}% faster than PySide6\")\n",
    "else:\n",
    "    print(f\"PySide6 is {abs(pct):.1f}% faster than GLFW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optional: Frame Rate Benchmark\n",
    "\n",
    "Measure rendering performance by timing slider updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_frame_rate(backend: str, data_path: str, n_frames: int = 100) -> dict:\n",
    "    \"\"\"Benchmark frame update times.\"\"\"\n",
    "    \n",
    "    script = f'''\n",
    "import time\n",
    "import os\n",
    "os.environ[\"WGPU_GUI_BACKEND\"] = \"{backend}\"\n",
    "\n",
    "import mbo_utilities as mbo\n",
    "import fastplotlib as fpl\n",
    "import numpy as np\n",
    "from mbo_utilities.graphics._processors import MboImageProcessor\n",
    "\n",
    "data = mbo.imread(r\"{data_path}\")\n",
    "\n",
    "ndim = data.ndim\n",
    "if ndim == 4:\n",
    "    slider_dim_names = (\"t\", \"z\")\n",
    "    window_funcs = (np.mean, None)\n",
    "    window_sizes = (1, None)\n",
    "elif ndim == 3:\n",
    "    slider_dim_names = (\"t\",)\n",
    "    window_funcs = (np.mean,)\n",
    "    window_sizes = (1,)\n",
    "else:\n",
    "    slider_dim_names = None\n",
    "    window_funcs = None\n",
    "    window_sizes = None\n",
    "\n",
    "iw = fpl.ImageWidget(\n",
    "    data=data,\n",
    "    processors=MboImageProcessor,\n",
    "    slider_dim_names=slider_dim_names,\n",
    "    window_funcs=window_funcs,\n",
    "    window_sizes=window_sizes,\n",
    "    histogram_widget=False,\n",
    "    figure_kwargs={{\"size\": (800, 800)}},\n",
    "    graphic_kwargs={{\"vmin\": -100, \"vmax\": 4000}},\n",
    ")\n",
    "iw.show()\n",
    "\n",
    "# Warm up\n",
    "for i in range(5):\n",
    "    iw.indices[\"t\"] = i\n",
    "    iw.figure.canvas.draw()\n",
    "\n",
    "# Benchmark\n",
    "n_frames = min({n_frames}, data.shape[0] - 1)\n",
    "start = time.perf_counter()\n",
    "for i in range(n_frames):\n",
    "    iw.indices[\"t\"] = i\n",
    "    iw.figure.canvas.draw()\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "try:\n",
    "    iw.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "fps = n_frames / elapsed\n",
    "print(f\"frames:{n_frames}\")\n",
    "print(f\"elapsed:{elapsed:.4f}\")\n",
    "print(f\"fps:{fps:.2f}\")\n",
    "'''\n",
    "    \n",
    "    print(f\"  Running {n_frames} frame updates...\", end=\" \")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, \"-c\", script],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        env={**dict(__import__(\"os\").environ), \"WGPU_GUI_BACKEND\": backend}\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr[:200]}\")\n",
    "        return {\"backend\": backend, \"fps\": 0, \"elapsed\": 0, \"frames\": 0}\n",
    "    \n",
    "    print(\"done\")\n",
    "    data = {}\n",
    "    for line in result.stdout.strip().split(\"\\n\"):\n",
    "        key, val = line.split(\":\")\n",
    "        data[key] = float(val)\n",
    "    \n",
    "    return {\n",
    "        \"backend\": backend,\n",
    "        \"fps\": data.get(\"fps\", 0),\n",
    "        \"elapsed\": data.get(\"elapsed\", 0),\n",
    "        \"frames\": int(data.get(\"frames\", 0)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run frame rate benchmarks (takes longer)\n",
    "\n",
    "# print(\"Benchmarking GLFW frame rate...\")\n",
    "# glfw_fps = benchmark_frame_rate(\"glfw\", TEST_DATA_PATH, n_frames=100)\n",
    "# print(f\"GLFW: {glfw_fps['fps']:.2f} FPS ({glfw_fps['frames']} frames in {glfw_fps['elapsed']:.2f}s)\")\n",
    "\n",
    "# print(\"\\nBenchmarking PySide6 frame rate...\")\n",
    "# pyside6_fps = benchmark_frame_rate(\"pyside6\", TEST_DATA_PATH, n_frames=100)\n",
    "# print(f\"PySide6: {pyside6_fps['fps']:.2f} FPS ({pyside6_fps['frames']} frames in {pyside6_fps['elapsed']:.2f}s)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
